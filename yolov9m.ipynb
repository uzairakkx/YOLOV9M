{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# âœ… YOLOv9m CCTV Fine-tuning Script\n# âœ… Train from pretrained yolov9m.pt on custom dataset\n# =========================================================\n\nimport os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\n\n# -----------------------------\n# Check versions\n# -----------------------------\nprint(\"âœ… Albumentations version:\", A.__version__)\nprint(\"âœ… Torch version:\", torch.__version__)\n\n# =========================================================\n# âœ… Set global seed for reproducibility\n# =========================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True  # faster training\n\n# =========================================================\n# âœ… Albumentations augmentations\n# =========================================================\ntrain_transform = A.Compose([\n    A.RandomResizedCrop(size=(640, 640), scale=(0.65, 1.0), ratio=(0.9, 1.1), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.12, rotate_limit=8,\n                       border_mode=cv2.BORDER_CONSTANT, value=0, p=0.7),\n    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.6),\n    A.HueSaturationValue(hue_shift_limit=8, sat_shift_limit=12, val_shift_limit=12, p=0.5),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.35),\n    A.RandomGamma(gamma_limit=(80, 120), p=0.4),\n    A.MotionBlur(blur_limit=(3, 7), p=0.35),\n    A.GaussNoise(var_limit=(5, 18), p=0.45),\n    A.Defocus(radius=(1, 3), p=0.30),\n    A.ImageCompression(quality_lower=30, quality_upper=70, p=0.40),\n    ToTensorV2()\n],\n    bbox_params=A.BboxParams(\n        format='yolo',\n        label_fields=['class_labels'],\n        min_visibility=0.10\n    )\n)\n\n# =========================================================\n# âœ… Dataset and save directory\n# =========================================================\ndata_yaml = \"/kaggle/input/vioelnceweapondetectiond/data.yaml\"\nsave_dir = \"/kaggle/working/yolov9m_cctv_aug\"\n\n# =========================================================\n# âœ… Load pretrained YOLOv9m\n# =========================================================\nmodel = YOLO(\"yolov9m.pt\")\n\n# Set light augmentations in model\nmodel.overrides['mosaic'] = 0.15\nmodel.overrides['mixup'] = 0.10\nmodel.overrides['copy_paste'] = 0.0\n\n# =========================================================\n# âœ… Apply Albumentations transforms during training\n# =========================================================\ndef custom_dataloader_hook(dataset):\n    dataset.transforms = train_transform\n    return dataset\n\nmodel.add_callback(\"on_fit_epoch_start\",\n                   lambda trainer: custom_dataloader_hook(trainer.train_loader.dataset))\n\n# =========================================================\n# âœ… Train from scratch on your dataset\n# =========================================================\nresults = model.train(\n    data=data_yaml,\n    epochs=200,\n    imgsz=640,\n    batch=8,\n    device=0,\n    project=save_dir,\n    name=\"yolov9m_cctv_high_accuracy\",\n    workers=2,\n    lr0=0.002,\n    optimizer=\"AdamW\",\n    pretrained=True,   # load yolov9m.pt\n    val=True,\n    exist_ok=True,\n    patience=25\n)\n\n# =========================================================\n# âœ… Validation\n# =========================================================\nmetrics = model.val()\nprint(\"\\nðŸ“Š Validation Metrics:\")\nprint(f\"Precision:   {metrics.box.pr:.4f}\")\nprint(f\"Recall:      {metrics.box.re:.4f}\")\nprint(f\"mAP50:       {metrics.box.map50:.4f}\")\nprint(f\"mAP50-95:    {metrics.box.map:.4f}\")\n\nprint(\"\\nâœ… Training Completed\")\n\n# =========================================================\n# âœ… Export to ONNX (optional)\n# =========================================================\nmodel.export(format=\"onnx\", dynamic=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# âœ… YOLOv9m CCTV Resume Training Script\n# âœ… (Continues from last.pt checkpoint)\n# =========================================================\n\nimport os\nimport cv2\nimport torch\nimport random\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\n\nprint(\"âœ… Albumentations\", A.__version__)\nprint(\"âœ… Torch\", torch.__version__)\n\n# =========================================================\n# âœ… GLOBAL SEED (for same augmentations)\n# =========================================================\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\n# Faster training mode (non-deterministic, but same aug)\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True\n\n# =========================================================\n# âœ… CCTV / SMALL OBJECT SAFE AUGMENTATION (same as before)\n# =========================================================\ntrain_transform = A.Compose([\n    A.RandomResizedCrop(size=(640, 640), scale=(0.65, 1.0), ratio=(0.9, 1.1), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0.04,\n        scale_limit=0.12,\n        rotate_limit=8,\n        border_mode=cv2.BORDER_CONSTANT,\n        value=0,\n        p=0.7\n    ),\n    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.6),\n    A.HueSaturationValue(hue_shift_limit=8, sat_shift_limit=12, val_shift_limit=12, p=0.5),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.35),\n    A.RandomGamma(gamma_limit=(80, 120), p=0.4),\n    A.MotionBlur(blur_limit=(3, 7), p=0.35),\n    A.GaussNoise(var_limit=(5, 18), p=0.45),\n    A.Defocus(radius=(1, 3), p=0.30),\n    A.ImageCompression(quality_lower=30, quality_upper=70, p=0.40),\n    ToTensorV2()\n],\n    bbox_params=A.BboxParams(\n        format='yolo',\n        label_fields=['class_labels'],\n        min_visibility=0.10\n    )\n)\n\n# =========================================================\n# âœ… Paths\n# =========================================================\ndata_yaml = \"/kaggle/input/vioelnceweapondetectiond/data.yaml\"\nsave_dir = \"/kaggle/working/yolov9m_cctv_aug_runs\"\nresume_checkpoint = os.path.join(save_dir, \"/kaggle/input/last-wights/last (11).pt\")\n\n# =========================================================\n# âœ… Load model from last.pt (resume)\n# =========================================================\nprint(f\"ðŸ”„ Resuming training from checkpoint: {resume_checkpoint}\")\nmodel = YOLO(resume_checkpoint)  # automatically loads optimizer, epoch, etc.\n\n# Light mix augmentation (same as before)\nmodel.overrides['mosaic'] = 0.15\nmodel.overrides['mixup'] = 0.10\nmodel.overrides['copy_paste'] = 0.0\n\n# =========================================================\n# âœ… Keep same augmentation hook\n# =========================================================\ndef custom_dataloader_hook(dataset):\n    dataset.transforms = train_transform\n    return dataset\n\nmodel.add_callback(\"on_fit_epoch_start\",\n                   lambda trainer: custom_dataloader_hook(trainer.train_loader.dataset))\n\n# =========================================================\n# âœ… RESUME TRAINING\n# =========================================================\nresults = model.train(\n    data=data_yaml,\n    epochs=200,                 # total target epochs (YOLO auto-continues from last one)\n    imgsz=640,\n    batch=8,\n    device=0,\n    project=save_dir,\n    name=\"yolov9m_cctv_high_accuracy\",\n    workers=2,\n    lr0=0.002,\n    optimizer=\"AdamW\",\n    pretrained=True,\n    val=True,\n    exist_ok=True,\n    patience=25,\n    resume=True                 # âœ… key flag to resume from last.pt\n)\n\n# =========================================================\n# âœ… VALIDATION (after resume)\n# =========================================================\nmetrics = model.val()\nprint(\"\\nðŸ“Š Validation Metrics:\")\nprint(f\"Precision:   {metrics.box.pr:.4f}\")\nprint(f\"Recall:      {metrics.box.re:.4f}\")\nprint(f\"mAP50:       {metrics.box.map50:.4f}\")\nprint(f\"mAP50-95:    {metrics.box.map:.4f}\")\n\nprint(\"\\nâœ… Training Resumed and Completed Successfully\")\n\n# =========================================================\n# âœ… Export to ONNX (optional)\n# =========================================================\nmodel.export(format=\"onnx\", dynamic=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}